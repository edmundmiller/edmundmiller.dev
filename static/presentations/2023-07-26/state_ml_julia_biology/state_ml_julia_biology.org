#+title: Exploring the State of Machine Learning for Biological Data
#+setupfile: ../setup.org
#+options: ^:{}
# SPC m e l O

# https://pretalx.com/juliacon2023/me/submissions/CSG8NU/

* Abstract :noexport:

Exploring the use of Julia, in analyzing biological data. Discussion of libraries and packages, challenges and opportunities of using machine learning on biological data, and examples of past and future applications.

* Description :noexport:

This talk, "Exploring the State of Machine Learning for Biological Data in Julia," will delve into the use of the high-performance programming language, Julia, in analyzing biological data. We will discuss various libraries and packages available in Julia, such as BioJulia and Flux.jl, and the benefits of using Julia for machine learning in the field of biology. Additionally, the challenges and opportunities that arise when using machine learning techniques on biological data, such as dealing with high-dimensional and heterogeneous data, will be addressed. The talk will also include examples of how machine learning has been applied to biological data in the past and what the future holds for this field.

* Introduction
** About me

- Phd Candidate @ University of Texas at Dallas
- nf-core maintainer

#+beamer: \pause

- Been Machine Learning curious since around 2017
#+beamer: \pause
- Never took Linear Algebra

#+beamer: \pause

- I had some questions
#+beamer: \pause
  - What are the trade offs of all these packages?
#+beamer: \pause
  - How do we load biological data? (Personal interest in genomics)

** Exploring the State of Machine Learning for Biological Data

#+begin_quote
Be the PR you want to see in the repo
#+end_quote
- Michael Lingelbach

** Exploring the State of Machine Learning for Biological Data

#+begin_quote
Be the +PR+ Talk you want to see +in the repo+ at Juliacon
#+end_quote

** Goal of /most/ Biologists
# scope

- We're not trying to create novel machine-learning models
- We're trying to apply these models in novel ways
- Then make biological inferences


** Old overview

- Ability to call Python and R packages
- What are all of these different ML Packages?
- Loading biological file formats
- Some pretty basic toy examples

** Then I had a thought...

#+beamer: \pause

What if I reproduce some analyses and recount what I learned along the way?

** mlf-core - Overview

[[https://raw.githubusercontent.com/mlf-core/mlf-core/master/docs/images/mlf_core_overview.png]] [fn:1:https://doi.org/10.1093/bioinformatics/btad164]

*** :B_note:
:PROPERTIES:
:BEAMER_env: note
:END:

#+begin_quote
Deterministic machine learning project templates based on MLflow.
#+end_quote

** mlf-core - Summary

[[https://raw.githubusercontent.com/mlf-core/mlf-core/master/docs/images/mlf_core_summary.png]]

** mlf-core - Concept

#+begin_src python
# Install mlf-core
$ pip install mlf-core

# Get an overview of all commands
$ mlf-core --help

# Check out all available templates
$ mlf-core list

# Get started and create your first project
$ mlf-core create
#+end_src

# https://www.mlf-core.com/static/assets/img/index_gifs/config_ML.gif

** mlf-core - Why do we care about Reproducibility?

- why does reproducibility mater in science?
  - compared to selling ads, chatbots
- Lives are at stake

*** :B_note:
:PROPERTIES:
:BEAMER_env: note
:END:
- Patients receiving a diagnosis
- Future scientists trying to reproduce the finding.

* lcep
** lcep - Overview

- Classifying cancerous liver samples from gene expression data.
- RNA-seq data
- Nice warm up, purpose was to demonstrate [[https://github.com/mlf-core/lcep-package][creating a python package using
  mlf-core]], and using the package in [[https://github.com/mlf-core/nextflow-lcep][a Nextflow pipeline]]
- [[https://github.com/Emiller88/state-of-ml-for-biology-julia/tree/main/lcep][Repo Link]]

** lcep - Dataloading - mlf-core

\small
#+begin_src python
def load_train_test_data(train_data, test_data):
    X_train, y_train, train_gene_names, train_sample_names = parse_tpm_table(train_data)

    # Convert to Numpy Arrays
    X_train_np = np.array(X_train)

    # Convert from Numpy Arrays to XGBoost Data Matrices
    dtrain = xgb.DMatrix(X_train_np, label=y_train)

    training_data = Dataset(X_train_np, y_train, dtrain, train_gene_names, train_sample_names)

    return training_data
#+end_src

** lcep - Dataloading

#+begin_src julia
train_url = "https://github.com/mlf-core/lcep/raw/master/data/train.tsv"
test_url = "https://github.com/mlf-core/lcep/raw/master/data/test.tsv"

train_data = DataFrame(CSV.File(download(train_url)))
test_data = DataFrame(CSV.File(download(test_url)))
#+end_src

- Note the lack of need to dance around with np arrays and XGBoost Data Matrices
  - The Julia XGBoost wrapper handles the conversion from DataFrames to DMatrix

*** :B_note:
:PROPERTIES:
:BEAMER_env: note
:END:

- Data was precleaned following
** lcep - Dataloading

#+attr_latex: :font \small :align llll
| Gene ID         | Gene Name | 0_SRR143622 | 1_bce80114-27b0-4318-9af1-d8fdf85ffd9c |
|-----------------+-----------+-------------+----------------------------------------|
| ENSG00000004975 | missing   |     14.2893 |                                18.7965 |
| ENSG00000005339 | missing   |     83.0387 |                                78.4725 |
| ENSG00000005884 | missing   |     2.70558 |                                11.0217 |
| ENSG00000006451 | missing   |     17.5549 |                                34.9154 |

** lcep - Data Cleaning - python

- Probably could have used pandas

#+begin_src python
def parse_tpm_table(input):
    X_train = []
    y_train = []
    gene_names = []
    sample_names = []
#+end_src

** lcep - Data Cleaning - python
#+beamer: \footnotesize
#+begin_src python
    with open(input, "r") as file:
        all_runs_info = next(file).split("\n")[0].split("\t")[2:]
        for run_info in all_runs_info:
            split_info = run_info.split("_")
            y_train.append(int(split_info[0]))
            sample_names.append(split_info[1])
        for line in file:
            splitted = line.split("\n")[0].split("\t")
            X_train.append([float(x) for x in splitted[2:]])
            gene_names.append(splitted[:2])

    X_train = [list(i) for i in zip(*X_train)]

    return X_train, y_train, gene_names, sample_names
#+end_src

** lcep - Data Cleaning - Julia

#+beamer: \footnotesize
#+begin_src julia
function clean_data(input)
    # Drop any rows that are 0s
    input_zeros = input[findall(x -> x != 0, names(input)), :]
    # Drop Gene Name col
    input_id = input_zeros[!, Not(2)]
    # Flip the dataframe
    input_flip = rename(permutedims(input_id, "Gene ID"), "Gene ID" => :status)

    # The 1_s(cancer) and 0_s(normal) are the labels
    # Split status column by _ and take the first
    transform(input_flip, :status => ByRow(x -> parse(Float64, split(x, "_")[1])) => :status)
end
#+end_src


** TODO lcep - Training

\small
#+begin_src python
booster = xgb.train(param, training_data.DM, dict_args['max_epochs'], evals=[(test_data.DM, 'test')], evals_result=results)

test_predictions = np.round(booster.predict(test_data.DM))
#+end_src

#+begin_src julia
train = (clean_train_data[:, 2:end], clean_train_data.status)
bst = xgboost(train; num_round=1000, param...)

test_predictions = predict(bst, clean_test_data)
#+end_src

# Calculate log metrics?

** GPUs


#+begin_src julia
using CUDA
X = cu(randn(1000, 3))
y = randn(1000)

dm = DMatrix(X, y)
XGBoost.isgpu(dm)  # true

xgboost((X, y), num_rounds=10)  # no need to use `DMatrix`
#+end_src

- [[https://github.com/ageron/julia_notebooks/blob/main/Julia_Colab_Notebook_Template.ipynb][ageron Julia notebooks Template]]
- Automatically downloads the CUDA toolkit for you

* sc-autoencoder
** sc-autoencoder - Overview

- 3000 [[https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.1.0/pbmc3k][Peripheral blood mononuclear cells (PBMCs)]] from 10x Genomics
- This time however they started from ~h5ad~
- However they used [[https://scanpy.readthedocs.io/en/stable/][scanpy]] to load and clean the data

** TODO Quick Single Cell aside :noexport:
** Dataloading - Attempt to replicate scanpy functionality in Julia


#+begin_src julia
using Muon

pbmc3k_url = "https://raw.githubusercontent.com/chanzuckerberg/cellxgene/main/example-dataset/pbmc3k.h5ad"

pmbc3k = readh5ad(download(pbmc3k_url))
#+end_src

- Muon is a part of [[https://scverse.org/][scverse]], the same group that wrote scanpy
- But it threw an error

\small
#+begin_src bash
ERROR: MethodError: no method matching read_dataframe(::HDF5.Dataset)
#+end_src

** Hacking on a package

#+begin_src julia
pkg> develop --local Muon
#+end_src

- Then there's a repo cloned at ~dev/Muon~!
- And added to the Project.toml for tracking
#+beamer: \pause
- You can do this in python but here it's Julia all the way down

# Add a Julia icon
*** :B_note:
:PROPERTIES:
:BEAMER_env: note
:END:

Sadly fixing this was outside of my paygrade currently. Left off trying to get
Muon to work. It's been fun to load a julia module and hack on it. Not sure if
that path is going to go anywhere, probably should just move on to scanpy.

** Loading Data using PythonCall - CondaPkg.jl

#+begin_src julia
julia> using CondaPkg
pkg> conda add_channel conda-forge
pkg> conda add scanpy python-igraph leidenalg
#+end_src

~CondaPkg.toml~
#+begin_src toml
channels = ["conda-forge"]

[deps]
leidenalg = ""
scanpy = ""
python-igraph = ""
#+end_src

** PythonCall and Pycall are different

- Doesn't have to support as much legacy
  - PythonCall supports Julia 1.6.1+ and Python 3.7+
  - PyCall supports Julia 0.7+ and Python 2.7+.
- Uses CondaPkg by default
- You can use them both at the same time if you needed to for some reason

** Loading Data using PythonCall

#+begin_src julia
using PythonCall

sc = pyimport("scanpy")

function preprocessing(adata)
    sc.pp.filter_cells(adata, min_genes=200)
    sc.pp.filter_genes(adata, min_cells=3)

    # Normalization and scaling:
    sc.pp.normalize_total(adata, target_sum=1e4)
    sc.pp.log1p(adata)
#+end_src

** Loading Data using PythonCall

#+begin_src julia
    # Identify highly-variable genes
    sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, subset=true)
    sc.pp.scale(adata, zero_center=true, max_value=3)
    x = adata.X
    # We don't need Tensorflow because Julia is fast enough I think?
    # data = tf.data.Dataset.from_tensor_slices((x, x))
    x = pyconvert(Array{Float32}, x)

    return [x, x], x
end

dataset, test_data = preprocessing(adata)
dataset = Flux.DataLoader(dataset, batchsize=32)
#+end_src


** TensorFlow Dataset Loading

#+begin_src python
data = tf.data.Dataset.from_tensor_slices((x, x))
#+end_src

- Realized once again there's no need to learn yet another library, can just use
  built-in Julia types

** DataToolkit

- Conda doesn't work well on NixOS
- Exported the ~x~ Matrix
#+begin_src julia
pkg> add DataToolkit
# }
(.)> init
(sc-autoencoder) data> add pbmc3k https://huggingface.co/datasets/emiller/pbmc3k/resolve/main/delim_file.txt
#+end_src

#+beamer: \pause

- Better practice would be to use the [[https://tecosaur.github.io/DataToolkitDocs/common/stable/loaders/julia/][Julia loader]]

** DataToolkit

#+begin_src conf-data-toml
[[pbmc3k]]
uuid = "2c0e014e-eea8-49e2-9916-6ab5d2df08b3"
description = "Preprocessed pmbc3k Single Cell dataset"

    [[pbmc3k.storage]]
    driver = "web"
    url = "https://huggingface.co/datasets/emiller/pbmc3k/resolve/main/delim_file.txt"

    [[pbmc3k.loader]]
    driver = "delim"
    delim = "\t"
    dtype = "Float32"
#+end_src

** DataToolkit

#+begin_src julia
using DataToolkit

dataset = d"pbmc3k"
#+end_src

** TODO DataToolkit - Want to learn more?

#+attr_latex: :width 0.7\linewidth
[[https://github.com/tecosaur/DataToolkit.jl/raw/main/docs/src/assets/logotype.svg]]
# [[https://git.tecosaur.net/assets/img/tree-gitea-themed.svg]]

Robust data management made simple: Introducing DataToolkit

Timothy Chapman

Friday, 07-28, 16:00–16:30 (US/Eastern), 32-123
** Flux

[[https://fluxml.ai/assets/logo.png]]
#+begin_center
The /Elegant/ Machine Learning Stack
#+end_center

** Flux - Model

#+attr_latex: :width 0.45\linewidth
[[file:./images/autoencoder_architecture.png]]

- Autoencoders are common in scRNA-seq data
  - Denoising of single cell data
  - predict perturbation responses

- mlf-core purpose was to show that *non-deterministic operations* can lead to
  significant differences in *latent space embeddings*
** Flux - DataLoader

#+begin_src julia
train_set = Flux.DataLoader((dataset, dataset), batchsize=256)
#+end_src

** Flux - Experiment Setup


#+begin_src julia
device = cpu # where will the calculations be performed?
L1, L2, L3 = 256, 128, 64 # layer dimensions
η = 0.01 # learning rate for ADAM optimization algorithm
batch_size = 100; # batch size for optimization
#+end_src

[[https://wildart.github.io/post/autoencoders/][Autoencoders blog post by wildart]]


** Flux - Build the Model


#+begin_src julia
enc1 = Dense(d, L1, relu)
enc2 = Dense(L1, L2, relu)
enc3 = Dense(L2, L3, relu)
dec4 = Dense(L3, L2, relu)
dec5 = Dense(L2, L1, relu)
dec6 = Dense(L1, d)
model = Chain(enc1, enc2, enc3, dec4, dec5, dec6) |> device
#+end_src

[[https://wildart.github.io/post/autoencoders/][Autoencoders blog post by wildart]]

** Flux - Training

#+begin_src julia
opt_state = Flux.setup(Adam(0.001), model)
for data in train_set
  # Unpack this element (for supervised training):
  input, label = data
  # Calculate the gradient of the objective
  # with respect to the parameters within the model:
  loss(A, B) = Flux.mae(model(A),B)
  grads = Flux.gradient(model) do m
      result = m(input)
      loss(result, label)
  end

  Flux.update!(opt_state, model, grads[1])
end
#+end_src

** Flux - ~train!~

#+begin_src julia
opt_state = Flux.setup(Adam(0.001), model)

loss(A, B) = Flux.mae(model(A),B)

@withprogress Flux.train!(model, train_set, opt_state) do m, x, y
  loss(m(x), y)
end
#+end_src

*** :B_note:
:PROPERTIES:
:BEAMER_env: note
:END:

- /Elegant/

* liver-ct-segmentation
** liver-ct-segmentation - Overview

- Liver-tumor segmentation of computed tomography scans using a U-Net model.
- The data in this challenge contains abdomen CT scans with contrast enhancement for liver lesions.

#+attr_latex: :width 0.7\linewidth
[[https://github.com/mlf-core/liver-ct-segmentation/raw/master/docs/images/u_net_lits.png]]

** liver-ct-segmentation - 3D U-Net architecture

[[https://github.com/mlf-core/liver-ct-segmentation/raw/master/docs/images/u_net_architecture.png]]
** liver-ct-segmentation - Dataset

#+begin_quote
The data set for LiTS was collected from 6 medical centres. The CT scans as well as the segmentations are provided as Nifti .nii files.
#+end_quote

- Of course there's a package for that [[https://github.com/JuliaNeuroscience/NIfTI.jl][JuliaNeuroscience/NIfTI.jl]] (It's 9 years
  old!)
** liver-ct-segmentation - model

- [[https://juliapackages.com/p/unet][UNet · Julia Packages]]
  - Written in Flux

#+begin_src julia
julia> u = Unet()
UNet:
  ConvDown(64, 64)
  ConvDown(128, 128)
  ConvDown(256, 256)
  ConvDown(512, 512)
  UNetConvBlock(1, 3)
  ...
  UNetConvBlock(1024, 1024)
  UNetUpBlock(1024, 512)
  ...
  UNetUpBlock(256, 64)
#+end_src

* Conclusion
** Things we touched upon along the way

- Simplicity of Dataloading
- Hacking on dependencies live
- Calling python packages from Julia
- DataToolkit
- Flux
- There's plenty of packages

** Things I didn't get answers to

- What would loading up BAMs like Deepvariant look like?
[[https://github.com/google/deepvariant/raw/r1.5/docs/images/inference_flow_diagram.svg]]

** Things I didn't get answers to

- What would [[https://github.com/FunctionLab/selene][FunctionLab/selene]] look like?
# TODO Add a cowbell emoji 🐮🔔
- More Determinism
  - ~Lux.jl~?
** Links
*** Left :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
Slides
[[file:./images/slides-qr-code.png]]

*** Right :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
[[https:link.edmundmiller.dev][link.edmundmiller.dev]]
[[file:~/src/personal/edmundmiller-dev/static/presentations/2023-07-26/links_qr.png]]
