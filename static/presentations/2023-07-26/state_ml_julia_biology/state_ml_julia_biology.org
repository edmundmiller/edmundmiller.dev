#+title: Exploring the State of Machine Learning for Biological Data
#+setupfile: ../setup.org
# SPC m e l O

# https://pretalx.com/juliacon2023/me/submissions/CSG8NU/

* Abstract :noexport:

Exploring the use of Julia, in analyzing biological data. Discussion of libraries and packages, challenges and opportunities of using machine learning on biological data, and examples of past and future applications.

* Description :noexport:

This talk, "Exploring the State of Machine Learning for Biological Data in Julia," will delve into the use of the high-performance programming language, Julia, in analyzing biological data. We will discuss various libraries and packages available in Julia, such as BioJulia and Flux.jl, and the benefits of using Julia for machine learning in the field of biology. Additionally, the challenges and opportunities that arise when using machine learning techniques on biological data, such as dealing with high-dimensional and heterogeneous data, will be addressed. The talk will also include examples of how machine learning has been applied to biological data in the past and what the future holds for this field.

* Exploring the State of Machine Learning for Biological Data

#+begin_quote
Be the PR you want to see in the repo
#+end_quote

* Exploring the State of Machine Learning for Biological Data

#+begin_quote
Be the +PR+ Talk you want to see +in the repo+ at Juliacon
#+end_quote

* About me

- Phd Candidate @ University of Texas at Dallas
- nf-core maintainer

#+beamer: \pause

- Been Machine Learning curious since around 2017
- Never took Linear Algebra

#+beamer: \pause

- I had some questions
  - What are the trade offs of all these packages?
  - How do we load biological data? (Personal interest in genomics)

* Goal of /most/ Biologists
# scope

- We're not trying to create novel machine-learning models
- We're trying to apply these models in novel ways
- Then make biological inferences


* Old overview

- Ability to call Python and R packages
- What are all of these different ML Packages?
- Loading biological file formats
- Some pretty basic toy examples

* Then I had a thought...

#+beamer: \pause

- What if I reproduce some analyses and recount what I learned along the way?

* mlf-core :ATTACH:
:PROPERTIES:
:ID:       65b35117-9044-4210-a3d6-0182a74bd75d
:END:

# FIXME add mlf-core logo

#+begin_quote
Deterministic machine learning project templates based on MLflow.
#+end_quote

* mlf-core - Concept

#+begin_src python
# Install mlf-core
$ pip install mlf-core

# Get an overview of all commands
$ mlf-core --help

# Check out all available templates
$ mlf-core list

# Get started and create your first project
$ mlf-core create
#+end_src

https://www.mlf-core.com/static/assets/img/index_gifs/config_ML.gif
* TODO mlf-core - Why do we care about Reproducibility?

- why does reproducibility mater in science?
  - compared to selling ads, chatbots
- Lives are at stake
#+beamer: \pause
  - Treatment
#+beamer: \pause
  - Future scientists who can't reproduce it.


* PROJ lcep
** lcep - Overview

- Classifying cancerous liver samples from gene expression data.
- RNA-seq data
- Nice warm up, purpose was to demonstrate [[https://github.com/mlf-core/lcep-package][creating a python package using
  mlf-core]], and using that in [[https://github.com/mlf-core/nextflow-lcep][a Nextflow pipeline]]
- [[https://github.com/Emiller88/state-of-ml-for-biology-julia/tree/main/lcep][Repo Link]]

** lcep - Dataloading - mlf-core

#+begin_src python
def load_train_test_data(train_data, test_data):
    X_train, y_train, train_gene_names, train_sample_names = parse_tpm_table(train_data)

    print(f'[bold blue]Number of total samples: {str(len(X_train))}')
    print(f'[bold blue]Number of cancer samples: {str(len([x for x in y_train if x == 1]))}')
    print(f'[bold blue]Number of normal samples: {str(len([x for x in y_train if x == 0]))}')

    # Convert to Numpy Arrays
    X_train_np = np.array(X_train)

    # Convert from Numpy Arrays to XGBoost Data Matrices
    dtrain = xgb.DMatrix(X_train_np, label=y_train)

    training_data = Dataset(X_train_np, y_train, dtrain, train_gene_names, train_sample_names)

    return training_data
#+end_src

** lcep - Dataloading

#+begin_src julia
using CSV, DataFrames

train_url = "https://github.com/mlf-core/lcep/raw/master/data/train.tsv"
test_url = "https://github.com/mlf-core/lcep/raw/master/data/test.tsv"

train_data = DataFrame(CSV.File(download(train_url)))
test_data = DataFrame(CSV.File(download(test_url)))
#+end_src

- Note the lack of need to dance around with np arrays and XGBoost Data Matrices
  - The Julia XGBoost wrapper handles the conversion from DataFrames to DMatrix

*** :B_note:
:PROPERTIES:
:BEAMER_env: note
:END:

- Data was precleaned following
** lcep - Dataloading

\tiny
#+begin_example
530×422 DataFrame
 Row │ Gene ID          Gene Name  1_bce80114-27b0-4318-9af1-d8fdf85ffd9c  0_SRR143622 ⋯     │ String15         Missing    Float64                                 Float64     ⋯─────┼──────────────────────────────────────────────────────────────────────────────────   1 │ ENSG00000002330    missing                              60.0329      41.3051    ⋯   2 │ ENSG00000002745    missing                               0.0          0.0622438
   3 │ ENSG00000004975    missing                              18.7965      14.2893
   4 │ ENSG00000005339    missing                              78.4725      83.0387
   5 │ ENSG00000005884    missing                              11.0217       2.70558   ⋯   6 │ ENSG00000005961    missing                               0.0994137    0.493808
   7 │ ENSG00000006451    missing                              34.9154      17.5549
#+end_example

** lcep - Data Cleaning - python
#+begin_src python
def parse_tpm_table(input):
    X_train = []
    y_train = []
    gene_names = []
    sample_names = []
    with open(input, "r") as file:
        all_runs_info = next(file).split("\n")[0].split("\t")[2:]
        for run_info in all_runs_info:
            split_info = run_info.split("_")
            y_train.append(int(split_info[0]))
            sample_names.append(split_info[1])
        for line in file:
            splitted = line.split("\n")[0].split("\t")
            X_train.append([float(x) for x in splitted[2:]])
            gene_names.append(splitted[:2])

    X_train = [list(i) for i in zip(*X_train)]

    return X_train, y_train, gene_names, sample_names
#+end_src

#+begin_src julia
function clean_data(input)
    # Drop any rows that are 0s
    input_zeros = input[findall(x -> x != 0, names(input)), :]
    # Drop Gene Name col
    input_id = input_zeros[!, Not(2)]
    # Flip the dataframe
    input_flip = rename(permutedims(input_id, "Gene ID"), "Gene ID" => :status)

    # The 1_s(cancer) and 0_s(normal) are the labels
    # Split status column by _ and take the first
    transform(input_flip, :status => ByRow(x -> parse(Float64, split(x, "_")[1])) => :status)
end
#+end_src

- Probably could have used pandas

** TODO lcep - training

#+begin_src python
booster = xgb.train(param, training_data.DM, dict_args['max_epochs'], evals=[(test_data.DM, 'test')], evals_result=results)

test_predictions = np.round(booster.predict(test_data.DM))
calculate_log_metrics(test_data.y, test_predictions)
#+end_src

#+begin_src julia
train = (clean_train_data[:, 2:end], clean_train_data.status)
bst = xgboost(train; num_round=1000, param...)

test_predictions = predict(bst, clean_test_data)
#+end_src

# Calculate log metrics?
** GPUs

- [[https://github.com/ageron/julia_notebooks/blob/main/Julia_Colab_Notebook_Template.ipynb][ageron/julia_notebooks Template]]

#+begin_src julia
using CUDA
X = cu(randn(1000, 3))
y = randn(1000)

dm = DMatrix(X, y)
XGBoost.isgpu(dm)  # true

xgboost((X, y), num_rounds=10)  # no need to use `DMatrix`
#+end_src

** TODO lcep - GPU :noexport:

* PROJ sc-autoencoder
**  sc-autoencoder - Overview

- 3000 [[https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.1.0/pbmc3k][Peripheral blood mononuclear cells (PBMCs)]] from 10x
- This time however they started from ~h5ad~
- However they used [[https://scanpy.readthedocs.io/en/stable/][scanpy]] to load and clean the data

** TODO Quick Single Cell aside :noexport:
** Dataloading - Attempt to replicate scanpy functionality in Julia


#+begin_src julia
using Muon

pbmc3k_url = "https://raw.githubusercontent.com/chanzuckerberg/cellxgene/main/example-dataset/pbmc3k.h5ad"

pmbc3k = readh5ad(download(pbmc3k_url))
#+end_src

- Muon is a part of [[https://scverse.org/][scverse]], the same group that wrote scanpy
- But it threw an error

#+begin_example
ERROR: MethodError: no method matching read_dataframe(::HDF5.Dataset)
#+end_example

** Hacking on a package

#+begin_src julia
pkg> develop --local Muon
#+end_src

- Then there's a repo cloned at ~dev/Muon~!
- And added to the Project.toml for tracking
#+beamer: \pause
- You can do this in python but here it's Julia all the way down

# Add a Julia icon
*** :B_note:
:PROPERTIES:
:BEAMER_env: note
:END:

Sadly fixing this was outside of my paygrade currently. Left off trying to get
Muon to work. It's been fun to load a julia module and hack on it. Not sure if
that path is going to go anywhere, probably should just move on to scanpy.

** Loading Data using PythonCall - CondaPkg.jl

#+begin_src julia
julia> using CondaPkg
pkg> conda add_channel conda-forge
pkg> conda add scanpy python-igraph leidenalg
#+end_src

~CondaPkg.jl~
#+begin_src toml
channels = ["conda-forge"]

[deps]
leidenalg = ""
scanpy = ""
python-igraph = ""
#+end_src

** Loading Data using PythonCall

#+begin_src julia
using PythonCall

sc = pyimport("scanpy")

function preprocessing(adata)
    sc.pp.filter_cells(adata, min_genes=200)
    sc.pp.filter_genes(adata, min_cells=3)

    # Normalization and scaling:
    sc.pp.normalize_total(adata, target_sum=1e4)
    sc.pp.log1p(adata)

    # Identify highly-variable genes
    sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, subset=true)
    sc.pp.scale(adata, zero_center=true, max_value=3)
    x = adata.X
    # We don't need Tensorflow because Julia is fast enough I think?
    # data = tf.data.Dataset.from_tensor_slices((x, x))
    x = pyconvert(Array{Float32}, x)

    return [x, x], x
end

dataset, test_data = preprocessing(adata)
dataset = Flux.DataLoader(dataset, batchsize=32)
#+end_src

#+begin_src python
data = tf.data.Dataset.from_tensor_slices((x, x))
#+end_src

- Realized once again there's no need to learn yet another library, can just use
  built-in Julia types

** PythonCall and Pycall are different

- Doesn't have to support as much legacy
  - PythonCall supports Julia 1.6.1+ and Python 3.7+
  - PyCall supports Julia 0.7+ and Python 2.7+.
- Uses CondaPkg by default
- You can use them both at the same time if you needed to for some reason

** Switch to DataToolkit

- Conda doesn't work well on NixOS
- Exported the Matrix
#+begin_src julia
pkg> add DataToolkit
# }
(.)> init
(sc-autoencoder) data> add pbmc3k https://huggingface.co/datasets/emiller/pbmc3k/resolve/main/delim_file.txt
#+end_src

#+beamer: \pause

- Better practice would be to use the [[https://tecosaur.github.io/DataToolkitDocs/common/stable/loaders/julia/][Julia loader]]

** Switch to DataToolkit

#+begin_src conf-data-toml
[[pbmc3k]]
uuid = "2c0e014e-eea8-49e2-9916-6ab5d2df08b3"
description = "Preprocessed pmbc3k Single Cell dataset"

    [[pbmc3k.storage]]
    driver = "web"
    url = "https://huggingface.co/datasets/emiller/pbmc3k/resolve/main/delim_file.txt"

    [[pbmc3k.loader]]
    driver = "delim"
    delim = "\t"
    dtype = "Float32"
#+end_src

** DataToolkit Aftermath

#+begin_src julia
using DataToolkit

dataset = d"pbmc3k"
#+end_src

** TODO DataToolkit - Want to learn more?
# Link to Teco's presentation

Join Teco Friday @ TODO

** TODO Flux

[[https://raw.githubusercontent.com/FluxML/fluxml.github.io/main/_assets/FluxGitHubPreview.png]]

** TODO Flux - Model

[[file:./images/autoencoder_architecture.png]]

- Autoencoders are common in scRNA-seq data
  - Denoising of single cell data
  - predict perturbation responses

- mlf-core purpose was to show that *non-deterministic operations* can lead to
  significant differences in *latent space embeddings*
** TODO Flux - DataLoader

#+begin_src julia
train_set = Flux.DataLoader((dataset, dataset), batchsize=256)
#+end_src

** TODO Flux - Experiment Setup


#+begin_src julia
device = cpu # where will the calculations be performed?
L1, L2, L3 = 256, 128, 64 # layer dimensions
η = 0.01 # learning rate for ADAM optimization algorithm
batch_size = 100; # batch size for optimization
#+end_src

# TODO cite
[[https://wildart.github.io/post/autoencoders/][Autoencoders by wildart]]

** Flux - Build the Model


#+begin_src julia
enc1 = Dense(d, L1, relu)
enc2 = Dense(L1, L2, relu)
enc3 = Dense(L2, L3, relu)
dec4 = Dense(L3, L2, relu)
dec5 = Dense(L2, L1, relu)
dec6 = Dense(L1, d)
model = Chain(enc1, enc2, enc3, dec4, dec5, dec6) |> device
#+end_src

# TODO cite
[[https://wildart.github.io/post/autoencoders/][Autoencoders by wildart]]

** Flux - Training

#+begin_src julia
opt_state = Flux.setup(Adam(0.001), model)

for data in train_set
  # Unpack this element (for supervised training):
  input, label = data
  # Calculate the gradient of the objective
  # with respect to the parameters within the model:
  loss(A, B) = Flux.mae(model(A),B)
  grads = Flux.gradient(model) do m
      result = m(input)
      loss(result, label)
  end

  # Update the parameters so as to reduce the objective,
  # according the chosen optimisation rule:
  Flux.update!(opt_state, model, grads[1])
end
#+end_src

** Flux - ~train!~

#+begin_src julia
opt_state = Flux.setup(Adam(0.001), model)

loss(A, B) = Flux.mae(model(A),B)

@withprogress Flux.train!(model, train_set, opt_state) do m, x, y
  loss(m(x), y)
end
#+end_src

*** :B_note:
:PROPERTIES:
:BEAMER_env: note
:END:

- /Elegant/

* PROJ liver-ct-segmentation
** TODO liver-ct-segmentation - Overview

- Liver-tumor segmentation of computed tomography scans using a U-Net model.
- The data in this challenge contains abdomen CT scans with contrast enhancement for liver lesions.
[[https://github.com/mlf-core/liver-ct-segmentation/raw/master/docs/images/u_net_lits.png]]

** TODO liver-ct-segmentation - Model

[[https://github.com/mlf-core/liver-ct-segmentation/raw/master/docs/images/u_net_architecture.png]]
** liver-ct-segmentation - Dataset

#+begin_quote
The data set for LiTS was collected from 6 medical centres. The CT scans as well as the segmentations are provided as Nifti .nii files.
#+end_quote

- Of course there's a package for that [[https://github.com/JuliaNeuroscience/NIfTI.jl][JuliaNeuroscience/NIfTI.jl]] (It's 9 years
  old!)
** liver-ct-segmentation - model

- [[https://juliapackages.com/p/unet][UNet · Julia Packages]]
  - Written in Flux

* Things we touched upon along the way

- Simplicity of Dataloading
- Hacking on dependencies live
- Calling python packages from Julia
- DataToolkit
- Flux
- There's a package for that!

* TODO Things I didn't get answers to
- What would loading up BAMs like Deepvariant look like?
- What would [[https://github.com/FunctionLab/selene][FunctionLab/selene]] look like
# TODO Add a cowbell emoji
- More Determinism
  - ~Lux.jl~?
* Future plans

- Implement liver-ct-segmentation
- Reproduce [[https://github.com/instadeepai/nucleotide-transformer/tree/main][nucleotide-transformer]]

* Conclusion :noexport:

#+beamer: \pause
- I need to read a few books on linear algebra

* Ideas :noexport:
** TODO Calling Selene
https://github.com/FunctionLab/selene/blob/master/tutorials/quickstart_training/quickstart_training.ipynb
https://github.com/FunctionLab/selene/blob/master/tutorials/getting_started_with_selene/getting_started_with_selene.ipynb
** scratch
*** What are all of these Packages?

- Flux.jl
- SciML
- MLJ
- fast.ai
- MLJFlux.jl

*** Loading biological file formats in a training data format

- FASTA
- BAM
- BED

*** Flux
https://fluxml.ai/Flux.jl/stable/gpu/
https://www.freecodecamp.org/news/deep-learning-with-julia/
*** Exploring the State of Machine Learning for Biological Data
**** Things I want to cover
- Loading file biological file formats

- Goal of Biologists using machine learning
- We're not trying to create novel model
- We're trying to apply these models in novel ways
- Then make biological inferences

**** Reproduction of mlf-core examples

- [[https://github.com/mlf-core/sc-autoencoder][An autoencoder for single cell data.]]
- [[https://github.com/mlf-core/lcep][Classifying cancerous liver samples from gene expression data.]]
- [[https://github.com/mlf-core/liver-ct-segmentation][Liver-tumor segmentation of computed tomography scans using a U-Net model.]]
