#+title: Exploring the State of Machine Learning for Biological Data
#+author: Edmund Miller
#+language: en
#+date: July 26th, 2023
#+exclude_tags: noexport
#+options: num:nil
#+options: toc:nil
#+startup: inlineimages
#+startup: beamer
#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [bigger]
#+beamer_frame_level: 2
# SPC m e l O

# https://pretalx.com/juliacon2023/me/submissions/CSG8NU/

* Abstract :noexport:

Exploring the use of Julia, in analyzing biological data. Discussion of libraries and packages, challenges and opportunities of using machine learning on biological data, and examples of past and future applications.

* Description :noexport:

This talk, "Exploring the State of Machine Learning for Biological Data in Julia," will delve into the use of the high-performance programming language, Julia, in analyzing biological data. We will discuss various libraries and packages available in Julia, such as BioJulia and Flux.jl, and the benefits of using Julia for machine learning in the field of biology. Additionally, the challenges and opportunities that arise when using machine learning techniques on biological data, such as dealing with high-dimensional and heterogeneous data, will be addressed. The talk will also include examples of how machine learning has been applied to biological data in the past and what the future holds for this field.

* Exploring the State of Machine Learning for Biological Data

#+begin_quote
Be the PR you want to see in the repo
#+end_quote

* Exploring the State of Machine Learning for Biological Data

#+begin_quote
Be the +PR+ Talk you want to see +in the repo+ at Juliacon
#+end_quote

* About me

- Phd Candidate @ University of Texas at Dallas
- nf-core maintainer

#+beamer: \pause

- Been Machine Learning curious since around 2017
- Never took Linear Algebra

#+beamer: \pause

- I had some questions
  - What are the trade offs of all these packages?
  - How do we load biological data? (Personal interest in genomics)

* Goal of /most/ Biologists
# scope

- We're not trying to create novel machine-learning models
- We're trying to apply these models in novel ways
- Then make biological inferences


* Old overview

- Ability to call Python and R packages
- What are all of these different ML Packages?
- Loading biological file formats
- Some pretty basic toy examples

* Then I had a thought...

#+beamer: \pause

- What if I reproduce some analyses and recount what I learned along the way?

* mlf-core :ATTACH:
:PROPERTIES:
:ID:       65b35117-9044-4210-a3d6-0182a74bd75d
:END:

# FIXME add mlf-core logo

#+begin_quote
Deterministic machine learning project templates based on MLflow.
#+end_quote

* mlf-core - Concept

#+begin_src python
# Install mlf-core
$ pip install mlf-core

# Get an overview of all commands
$ mlf-core --help

# Check out all available templates
$ mlf-core list

# Get started and create your first project
$ mlf-core create
#+end_src

https://www.mlf-core.com/static/assets/img/index_gifs/config_ML.gif
* TODO mlf-core - Why do we care about Reproducibility?

- why does reproducibility mater in science?
  - compared to selling ads, chatbots
- Lives are at stake
#+beamer: \pause
  - Treatment
#+beamer: \pause
  - Future scientists who can't reproduce it.


* PROJ lcep
** lcep - Overview

- Classifying cancerous liver samples from gene expression data.
- 3000 [[https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.1.0/pbmc3k][Peripheral blood mononuclear cells (PBMCs)]]
- Nice warm up, purpose was to demonstrate [[https://github.com/mlf-core/lcep-package][creating a python package using
  mlf-core]], and using that in [[https://github.com/mlf-core/nextflow-lcep][a Nextflow pipeline]]

** lcep- Dataloading - mlf-core

#+begin_src python
def load_train_test_data(train_data, test_data):
    X_train, y_train, train_gene_names, train_sample_names = parse_tpm_table(train_data)

    print(f'[bold blue]Number of total samples: {str(len(X_train))}')
    print(f'[bold blue]Number of cancer samples: {str(len([x for x in y_train if x == 1]))}')
    print(f'[bold blue]Number of normal samples: {str(len([x for x in y_train if x == 0]))}')

    # Convert to Numpy Arrays
    X_train_np = np.array(X_train)

    # Convert from Numpy Arrays to XGBoost Data Matrices
    dtrain = xgb.DMatrix(X_train_np, label=y_train)

    training_data = Dataset(X_train_np, y_train, dtrain, train_gene_names, train_sample_names)

    return training_data
#+end_src

** lcep - Dataloading

#+begin_src julia
using CSV, DataFrames

train_url = "https://github.com/mlf-core/lcep/raw/master/data/train.tsv"
test_url = "https://github.com/mlf-core/lcep/raw/master/data/test.tsv"

train_data = DataFrame(CSV.File(download(train_url)))
test_data = DataFrame(CSV.File(download(test_url)))
#+end_src

- Note the lack of need to dance around with np arrays and XGBoost Data Matrices
  - The Julia XGBoost wrapper handles the conversion from DataFrames to DMatrix

** lcep - Dataloading

\tiny
#+begin_example
530×422 DataFrame
 Row │ Gene ID          Gene Name  1_bce80114-27b0-4318-9af1-d8fdf85ffd9c  0_SRR143622 ⋯     │ String15         Missing    Float64                                 Float64     ⋯─────┼──────────────────────────────────────────────────────────────────────────────────   1 │ ENSG00000002330    missing                              60.0329      41.3051    ⋯   2 │ ENSG00000002745    missing                               0.0          0.0622438
   3 │ ENSG00000004975    missing                              18.7965      14.2893
   4 │ ENSG00000005339    missing                              78.4725      83.0387
   5 │ ENSG00000005884    missing                              11.0217       2.70558   ⋯   6 │ ENSG00000005961    missing                               0.0994137    0.493808
   7 │ ENSG00000006451    missing                              34.9154      17.5549
#+end_example

** lcep - Data Cleaning - python
#+begin_src python
def parse_tpm_table(input):
    X_train = []
    y_train = []
    gene_names = []
    sample_names = []
    with open(input, "r") as file:
        all_runs_info = next(file).split("\n")[0].split("\t")[2:]
        for run_info in all_runs_info:
            split_info = run_info.split("_")
            y_train.append(int(split_info[0]))
            sample_names.append(split_info[1])
        for line in file:
            splitted = line.split("\n")[0].split("\t")
            X_train.append([float(x) for x in splitted[2:]])
            gene_names.append(splitted[:2])

    X_train = [list(i) for i in zip(*X_train)]

    return X_train, y_train, gene_names, sample_names
#+end_src

#+begin_src julia
function clean_data(input)
    # Drop any rows that are 0s
    input_zeros = input[findall(x -> x != 0, names(input)), :]
    # Drop Gene Name col
    input_id = input_zeros[!, Not(2)]
    # Flip the dataframe
    input_flip = rename(permutedims(input_id, "Gene ID"), "Gene ID" => :status)

    # The 1_s(cancer) and 0_s(normal) are the labels
    # Split status column by _ and take the first
    transform(input_flip, :status => ByRow(x -> parse(Float64, split(x, "_")[1])) => :status)
end
#+end_src

- Probably could have used pandas

** TODO lcep - training

#+begin_src python
booster = xgb.train(param, training_data.DM, dict_args['max_epochs'], evals=[(test_data.DM, 'test')], evals_result=results)

test_predictions = np.round(booster.predict(test_data.DM))
calculate_log_metrics(test_data.y, test_predictions)
#+end_src

#+begin_src julia
train = (clean_train_data[:, 2:end], clean_train_data.status)
bst = xgboost(train; num_round=1000, param...)

test_predictions = predict(bst, clean_test_data)
#+end_src

# Calculate log metrics?
** GPUs

- [[https://github.com/ageron/julia_notebooks/blob/main/Julia_Colab_Notebook_Template.ipynb][ageron/julia_notebooks Template]]

#+begin_src julia
using CUDA
X = cu(randn(1000, 3))
y = randn(1000)

dm = DMatrix(X, y)
XGBoost.isgpu(dm)  # true

xgboost((X, y), num_rounds=10)  # no need to use `DMatrix`
#+end_src

** TODO lcep - GPU :noexport:

* PROJ sc-autoencoder
* PROJ liver-ct-segmentation
* TODO Using Julia in Collab
https://colab.research.google.com/github/ageron/julia_notebooks/blob/master/Julia_Colab_Notebook_Template.ipynb
Throw a screenshot in here
** TODO Hooking up to CUDA
https://fluxml.ai/Flux.jl/stable/gpu/
* TODO Things I didn't get answers to
- What would loading up BAMs like Deepvariant look like?
- What would [[https://github.com/FunctionLab/selene][FunctionLab/selene]] look like
* TODO Where I'd love to follow up
https://github.com/instadeepai/nucleotide-transformer/tree/main
* Conclusion

#+beamer: \pause
- I need to read a few books on linear algebra

* Ideas :noexport:
** TODO Calling Selene
https://github.com/FunctionLab/selene/blob/master/tutorials/quickstart_training/quickstart_training.ipynb
https://github.com/FunctionLab/selene/blob/master/tutorials/getting_started_with_selene/getting_started_with_selene.ipynb
** TODO Reproduce https://github.com/instadeepai/nucleotide-transformer
** scratch
*** What are all of these Packages?

- Flux.jl
- SciML
- MLJ
- fast.ai
- MLJFlux.jl

*** Loading biological file formats in a training data format

- FASTA
- BAM
- BED

*** Flux
https://fluxml.ai/Flux.jl/stable/gpu/
https://www.freecodecamp.org/news/deep-learning-with-julia/
*** Exploring the State of Machine Learning for Biological Data
**** Things I want to cover
- Loading file biological file formats

- Goal of Biologists using machine learning
- We're not trying to create novel model
- We're trying to apply these models in novel ways
- Then make biological inferences

**** Reproduction of mlf-core examples

- [[https://github.com/mlf-core/sc-autoencoder][An autoencoder for single cell data.]]
- [[https://github.com/mlf-core/lcep][Classifying cancerous liver samples from gene expression data.]]
- [[https://github.com/mlf-core/liver-ct-segmentation][Liver-tumor segmentation of computed tomography scans using a U-Net model.]]
